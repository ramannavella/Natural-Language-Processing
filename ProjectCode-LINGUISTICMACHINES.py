# -*- coding: utf-8 -*-
"""LINGUISTICMACHINES_Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T4F2-ixGXX0PlflMMqjvfpbrNKCfiImw
"""

import spacy
from nltk.corpus import wordnet as wn
from spacy import displacy
from IPython.core.display import display, HTML
import nltk
nltk.download('wordnet')

TokenNLP = spacy.load("en_core_web_sm")

SentenceNLP = spacy.load("en_core_web_sm", disable=["tagger", "ner"])
SentenceNLP.pipeline

outputfile=open("outputfile.txt","w")

# Task 1- To read the dataset from the local system
from google.colab import files
uploaded=files.upload()
filename=None
for name, data in uploaded.items():
  filename=name

print(filename)

# Task 1- The dataset can also be read directly from collab
name=input("enter file name if you uploaded directly from the left")
filename=name+".txt"

# splitting the dataset into sentences
with open(filename,encoding='utf-8-sig') as fd:
  lines = fd.read().split("\n\n")
  #lines = fd.read().splitlines()
  while '' in lines:
    lines.remove('')
print("The Document is split by \\n\\n and appends each paragraph to the list:")
print(lines[0])

# Pre-processing the sentence to remove annotations and extract the entities and relationship
import re

sentences = []
entity1 = []
entity2 = []
rel = []
direction = []
in_between_list = []
count = 0

for l in lines:
  # Extracting entity 1
  entity = re.findall(r'\s*<e1>([^<]*)</e1>', l)
  
  if not entity:
    entity1.append("")
  else:
    entity1.append(entity[0])

  # Extracting entity 2
  entity = re.findall(r'\s*<e2>([^<]*)\</e2>', l)
  
  if not entity:
    entity2.append("")
  else:
    entity2.append(entity[0])
  
  # Extracting the words in between the entities
  in_bet_words = re.findall(r'</e1>([^<]*)\<e2>', l)
  in_between_list.append(in_bet_words[0])
 
  #Extracting the relation
  relation = re.split('"\\n',l)[1]
  
  # Extracting the direction
  dir = relation[relation.find("(")+1:relation.find(")")]

  # Pre-processing to remove the annotations 
  l = l.split('"',1)[1].lstrip()
  l = l.rsplit('"',1)[0]
  l = l.replace('<e1>','')
  l = l.replace('<e2>','')
  l = l.replace('</e1>','')
  l = l.replace('</e2>','')
  sentences.append(l)
  rel.append(relation)
  direction.append(dir)

import en_core_web_sm
nlp = en_core_web_sm.load()

# Task 2 of the project

outputfile=open("outputfile2.txt","w")
for sent in sentences:
    tokendoc = TokenNLP(sent)
    tokenlist=[]
    lemmalist=[]
    poslist=[]
    taglist=[]
    depedenlist=[]
    for token in tokendoc:
      tokenlist.append(token.text)
      lemmalist.append(token.lemma_)
      poslist.append(token.pos_)
      taglist.append(token.tag_)
      depedenlist.append(token.dep_)
    
    print("Sentence:")
    outputfile.writelines("Sentence:\n")
    print(sent)
    print(sent)
    outputfile.writelines(sent+"\n")
    print("\n")

    print("the tokens of the sentence are:")
    outputfile.writelines("the tokens of the sentence are:\n")
    print(tokenlist)
    outputfile.writelines(str(tokenlist)+"\n")

    print("the lemmas of the sentence are")
    outputfile.writelines("the lemmas of the sentence are:\n")
    print(lemmalist)
    outputfile.writelines(str(lemmalist)+"\n")

    print("the POS TAGS of the sentence are")
    outputfile.writelines("the POS TAGS of the sentence are:\n")
    print(poslist)
    outputfile.writelines(str(poslist)+"\n")

    print("the DEP TAGS of the sentence are")
    outputfile.writelines("the DEP TAGS of the sentence are:\n")
    print(depedenlist)
    outputfile.writelines(str(depedenlist)+"\n")
    html = displacy.render(tokendoc, style="dep")

    print("the NER Tags of the sentence are")
    doc = nlp(sent)
    outputfile.writelines("The NER Tags of the sentence are:\n")
    outputfile.writelines(str([(X.text, X.label_) for X in doc.ents])+"\n")
    print([(X.text, X.label_) for X in doc.ents])
    print("-------------TOKEN DETAILS----------------")
    outputfile.writelines("-------------TOKEN DETAILS----------------\n")
    for token in tokendoc:
      print("token:")
      outputfile.writelines("tokens:\n")
      print(token.text+" and lemma is "+ str(token.lemma_))
      outputfile.writelines(token.text+"  and lemma is "+ str(token.lemma_)+"\n")
      print("-------")
      outputfile.writelines("-------\n")
      for ss in wn.synsets(token.lemma_):
        print("SYSNSET:"+str(ss))
        outputfile.writelines("SYSNSET:"+str(ss)+"\n")
        print ("hypernyms: "+str(ss.hypernyms())+" hyponyms: " + str(ss.hyponyms())+" holonyms: " + str(ss.member_holonyms())+" meronyms: "+ str(ss.part_meronyms()))
        outputfile.writelines("hypernyms: "+str(ss.hypernyms())+" hyponyms: "+str(ss.hyponyms())+" holonyms: "+str(ss.member_holonyms())+" meronyms: "+str(ss.part_meronyms())+"\n")
      print("-----------------------------------")
      outputfile.writelines("-----------------------------------\n")


    display(HTML(html))
    
    print("***************************************************************")
    outputfile.writelines("***************************************************************\n")


outputfile.close()

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from spacy import displacy

# Extracting the part of speech tags of the entities

itr = 0
e1_pos = []
e2_pos = []

from itertools import groupby

def all_equal(iterable):
    g = groupby(iterable)
    return next(g, True) and not next(g, False)

for p in sentences:
  pos_list1 = []
  pos_list2 = []
  entity1_words = []
  entity1_word = entity1[itr]
  entity2_words = []
  entity2_word = entity2[itr]
  token2doc = nltk.word_tokenize(p)
  token3doc = nltk.pos_tag(token2doc)

  for token in token3doc:
    if(token[0] in entity1_word):
      pos_list1.append(token[1])
    if(token[0] in entity2_word):
      pos_list2.append(token[1])
      
  if((entity1[itr])):
      e1_pos.append(pos_list1)
  else:
    e1_pos.append('')
  if((entity2[itr])):
      e2_pos.append(pos_list2)
  else:
    e2_pos.append('')
  itr = itr + 1

# Extracting the dep tags of the entities

nlp=spacy.load('en_core_web_sm')

itr2 = 0
e1_dep = []
e2_dep = []

from itertools import groupby

def all_equal(iterable):
    g = groupby(iterable)
    return next(g, True) and not next(g, False)

for p in sentences:
  dep_list1 = []
  dep_list2 = []
  entity1_words = []
  entity1_words = entity1[itr2].replace('-', ' ').split()
  entity2_words = []
  entity2_words = entity2[itr2].replace('-', ' ').split()

  for token in nlp(p):
    if(token.text in entity1_words):
      dep_list1.append(token.dep_)
    if(token.text in entity2_words):
      dep_list2.append(token.dep_)

  if((entity1[itr2])):
    if(all_equal(dep_list1)):
      e1_dep.append(dep_list1[0])
    else:
      e1_dep.append(dep_list1[-1])
  else:
    e1_dep.append('')
  if((entity2[itr2])):
    if(all_equal(dep_list2)):
      e2_dep.append(dep_list2[0])
    else:
      e2_dep.append(dep_list2[-1])
  else:
    e2_dep.append('')
  itr2 = itr2 + 1

rel_wo_dir = []
for r in rel:
  a = r.split('(')[0]
  if(r.find('Other') == -1):
    rel_wo_dir.append(a)
  else:
    a = 'Other'
    rel_wo_dir.append(a)

# print(len(rel_wo_dir))
# print(len(entity1))
# print(len(entity2))
# print(len(e1_pos))
# print(len(e2_pos))
# print(len(e1_dep))
# print(len(e2_dep))
# print(len(in_between_list))

# Task 3 of the project:

matches_list = []
relation_direction = []
for var in in_between_list:
  matches_list.append('')
  relation_direction.append('')

# Message-Topic Component
from spacy.matcher import Matcher
unresolved_matcher = Matcher(nlp.vocab)

#defining rules to detect Message-Topic relation examples
#handling message-topic patterns:

v = 0
# unresolved_list = ['Instrument-Agency','Message-Topic']
unresolved_pattern = [{'POS': 'VERB'}]
                      
unresolved_matcher.add('UNRESOLVED_MATCHER', None, unresolved_pattern)

for var in sentences:
  entity1_words = []
  entity1_words = entity1[v].split()
  entity2_words = []
  entity2_words = entity2[v].split()
  e1_flag = 0
  e2_flag = 0
  verb_flag = 0

  for token in nlp(var):
    if(token.text in entity1_words):
      if(token.pos_ == 'NOUN'):
        e1_flag = 1
    if(token.text in entity2_words):
      if(token.pos_ == 'NOUN'):
        e2_flag = 1
  unresolved_doc = nlp(in_between_list[v])
  matches = unresolved_matcher(unresolved_doc)
  if(len(matches)>=1):
    verb_flag = 1
  if((e1_flag == 1) and (e2_flag == 1) and (verb_flag == 1)):
    matches_list[v] = 'Message-Topic'
    if(e1_dep[v] == 'nsubjpass' and e2_dep[v] == 'pobj'):
      relation_direction[v] = 'Message-Topic(e2,e1)'
    elif(e1_dep[v] == 'nsubjpass' and e2_dep[v] == 'conj'):
      relation_direction[v] = 'Message-Topic(e2,e1)'
    else:
      relation_direction[v] = 'Message-Topic(e1,e2)'
  v = v+1

# Instrument-Agency Component
from spacy.matcher import Matcher
ia_matcher = Matcher(nlp.vocab)

#defining rules to detect Instrument-Agency relation examples
#handling Instrument-Agency patterns:

v = 0
ia_pattern1 = [{'LEMMA': 'with'}, {'POS': 'DET'}]
ia_pattern2 = [{'LEMMA': 'by'}, {'LEMMA': 'using'}]
ia_pattern3 = [{'LEMMA': 'use'}, {'POS': 'DET'}]
ia_pattern4 = [{'LEMMA': 'using'}, {'POS': 'DET'}]

ia_matcher.add('INSTRUMENT_AGENCY_PATTERN', None, ia_pattern1)
ia_matcher.add('INSTRUMENT_AGENCY_PATTERN', None, ia_pattern2)
ia_matcher.add('INSTRUMENT_AGENCY_PATTERN', None, ia_pattern3)
ia_matcher.add('INSTRUMENT_AGENCY_PATTERN', None, ia_pattern4)

for var in sentences:
  entity1_words = []
  entity1_words = entity1[v].split()
  entity2_words = []
  entity2_words = entity2[v].split()
  e1_flag = 0
  e2_flag = 0
  verb_flag = 0

  for token in nlp(var):
    if(token.text in entity1_words):
      if(token.pos_ == 'NOUN'):
        e1_flag = 1
    if(token.text in entity2_words):
      if(token.pos_ == 'NOUN'):
        e2_flag = 1
  ia_doc = nlp(in_between_list[v])
  matches = ia_matcher(ia_doc)
  if(len(matches)>=1):
    verb_flag = 1
  if((e1_flag == 1) and (e2_flag == 1) and (verb_flag == 1)):
    matches_list[v] = 'Instrument-Agency'
    if(e1_dep[v] == 'nsubj'):
      if(e2_dep[v] == 'pobj' or e2_dep[v] == 'dobj'):
        relation_direction[v] = 'Instrument-Agency(e2,e1)'
      else:
        relation_direction[v] = 'Instrument-Agency(e1,e2)'
    elif(e1_dep[v] == 'compound'):
      if(e2_dep[v] == 'pobj'):
        relation_direction[v] = 'Instrument-Agency(e2,e1)'
      else:
        relation_direction[v] = 'Instrument-Agency(e1,e2)'
    else:
      relation_direction[v] = 'Instrument-Agency(e1,e2)'
  v = v+1

# Product-Producer Component

#defining rules to detect Product-Producer relation examples
#handling Product-Producer patterns:
itr = 0

product_producer_set = set()
product_producer_set.add('made')
product_producer_set.add('discovered')
product_producer_set.add('discover')
product_producer_set.add('found')
product_producer_set.add('created')
product_producer_set.add('create')
product_producer_set.add('invented')
product_producer_set.add('invent')
product_producer_set.add('developed')
product_producer_set.add('develop')
product_producer_set.add('order')
product_producer_set.add('manufactured')
product_producer_set.add('make')

for var in sentences:
  wordnet_set=set()
  doc = nlp(var)
  for token in doc:
    if(token.pos_ == 'VERB'):
      syn_list=wn.synsets(token.text)
      syn_list=[word for word in syn_list if word.name().split('.')[1]=='v']
      for i in range(0,len(syn_list)):
        for x in (val for val in syn_list[i].hypernyms()):
          x_val = x._name.split('.')[0]
          wordnet_set.add(x_val)
        for x in (val for val in syn_list[i].root_hypernyms()):
          x_val = x._name.split('.')[0]
          wordnet_set.add(x_val)
  # print(i)
  # print(wordnet_set)
  if(len(product_producer_set.difference(wordnet_set)) <= 12):
    matches_list[itr] = 'Product-Producer'
    if(e1_dep[itr] == 'nsubjpass'):
      if(e2_dep[itr] == 'pobj'):
        relation_direction[itr] = 'Product-Producer(e1,e2)'
      else:
        relation_direction[itr] = 'Product-Producer(e2,e1)'
    elif(e1_dep[itr] == 'dobj'):
      if(e2_dep[itr] == 'pobj' or e2_dep[itr] == 'nsubj'):
        relation_direction[itr] = 'Product-Producer(e1,e2)'
      else:
        relation_direction[itr] = 'Product-Producer(e2,e1)'
    elif(e1_dep[itr] == 'compound'):
      relation_direction[itr] = 'Product-Producer(e1,e2)'
    elif(e1_dep[itr] == 'attr'):
      if(e2_dep[itr] == 'pobj'):
        relation_direction[itr] = 'Product-Producer(e1,e2)'
      else:
        relation_direction[itr] = 'Product-Producer(e2,e1)'
    elif(e1_dep[itr] == 'nmod'):
      if(e2_dep[itr] == 'appos' or e2_dep[itr] == 'conj'):
        relation_direction[itr] = 'Product-Producer(e1,e2)'
      else:
        relation_direction[itr] = 'Product-Producer(e2,e1)'
    elif(e1_dep[itr] == 'pobj'):
      relation_direction[itr] = 'Product-Producer(e2,e1)'
    elif(e1_dep[itr] == 'nsubj'):
      relation_direction[itr] = 'Product-Producer(e2,e1)'
    else:
      relation_direction[itr] = 'Product-Producer(e2,e1)'
  itr = itr+1

# Member-Collection Component

from spacy.matcher import Matcher
mem_collec_matcher = Matcher(nlp.vocab)

#defining rules to detect Member-Collection relation examples
#handling Member-Collection patterns:
# Member Collection Patterns:

mc_pattern1 = [{'LEMMA': 'of'}, {'POS': 'NOUN'}]
mc_pattern2 = [{'LEMMA': 'of'}, {'POS': 'PROPN'}]
mc_pattern3 = [{'LEMMA': 'collect'}]
mc_pattern4 = [{'LEMMA': 'of'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}]

mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern1)
mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern2)
mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern3)
mem_collec_matcher.add('MEMBER_COLLECTION_PATTERN', None, mc_pattern4)

v = 0
e1_dep_set = set()
e1_dep_set.add('nsubj')
e1_dep_set.add('attr')
e1_dep_set.add('pobj')
e1_dep_set.add('nsubjpass')
e1_dep_set.add('dobj')
e1_dep_set.add('conj')

for var in in_between_list:
  mc_doc = nlp(var)
  matches = mem_collec_matcher(mc_doc)
  if(len(matches)>=1):
    matches_list[v] = 'Member-Collection'
    if((e2_dep == 'pobj') and (e1_dep in e1_dep_set)):
      relation_direction[v] = 'Member-Collection(e2,e1)'
    elif(e1_dep == 'nsubjpass'):
      relation_direction[v] = 'Member-Collection(e1,e2)'
    else:
      relation_direction[v] = 'Member-Collection(e2,e1)'
  v = v + 1

# Component-Whole Component
from spacy.matcher import Matcher
comp_whole_matcher = Matcher(nlp.vocab)


#defining rules to detect Component-Whole relation examples
#handling Component-Whole patterns:
# Component Whole Patterns:
cw_pattern1 = [{'LEMMA': 'of'}, {'POS': 'DET'}]
cw_pattern2 = [{'LEMMA': 'in'}, {'POS': 'DET'}]
cw_pattern3 = [{'POS': 'VERB'}, {'LEMMA': 'into'}]
cw_pattern4 = [{'LEMMA': 'inside'}, {'POS': 'DET'}]
cw_pattern5 = [{'LEMMA': 'comprises'}]
# cw_pattern4 = [{'LEMMA': 'of'}]

comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern1)
comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern2)
comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern3)
comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern4)
comp_whole_matcher.add('COMPONENT_WHOLE_PATTERN', None, cw_pattern5)

v = 0
for var in in_between_list:
  cw_doc = nlp(var)
  matches = comp_whole_matcher(cw_doc)
  if(len(matches)>=1):
    matches_list[v] = 'Component-Whole'
    if(e2_dep[v] == 'compound'):
      relation_direction[v] = 'Component-Whole(e1,e2)'
    elif(e1_dep[v] == 'compound'):
      relation_direction[v] = 'Component-Whole(e2,e1)'
    elif(e1_dep[v] == 'nsubjpass'):
      if(e2_dep[v] == 'pobj' or e2_dep[v] == 'nsubj'):
        relation_direction[v] = 'Component-Whole(e1,e2)'
      else:
        relation_direction[v] = 'Component-Whole(e2,e1)'
    elif(e1_dep[v] == 'nsubj'):
      if(e2_dep[v] == 'pobj'):
        relation_direction[v] = 'Component-Whole(e1,e2)'
      else:
        relation_direction[v] = 'Component-Whole(e2,e1)'
    elif(e1_dep[v] == 'dobj'):
      relation_direction[v] = 'Component-Whole(e1,e2)'
    elif(e1_dep[v] == 'pobj'):
      if(e2_dep[v] == 'dobj' or e2_dep[v] == 'conj'):
        relation_direction[v] = 'Component-Whole(e2,e1)'
      else:
        relation_direction[v] = 'Component-Whole(e2,e1)'
    else:
      relation_direction[v] = 'Component-Whole(e1,e2)'
  v = v + 1

# Content-Container Component
from spacy.matcher import Matcher
cont_cont_matcher = Matcher(nlp.vocab)

#defining rules to detect Content-Container relation examples
#handling Content-Container patterns:
# Content-Container Patterns:
cc_pattern1 = [{'POS': 'VERB'}, {'LEMMA': 'in'}, {'LEMMA': 'a'}]
cc_pattern2 = [{'POS': 'VERB'}, {'LEMMA': 'inside'}, {'POS': 'DET'}]
cc_pattern3 = [{'LEMMA': 'in'}, {'LEMMA': 'a'}]
cc_pattern4 = [{'LEMMA': 'contain'}]
cc_pattern5 = [{'LEMMA': 'content'}]

cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern1)
cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern2)
cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern3)
cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern4)
cont_cont_matcher.add('CONTENT_CONTAINER_PATTERN', None, cc_pattern5)

v = 0
for var in in_between_list:
  cc_doc = nlp(var)
  matches = cont_cont_matcher(cc_doc)
  if(len(matches)>=1):
    matches_list[v] = 'Content-Container'
    if(e1_dep[v] == 'nsubj' and e1_dep[v] == 'dobj'):
      relation_direction[v] = 'Content-Container(e2,e1)'
    else:
      relation_direction[v] = 'Content-Container(e1,e2)'
  v = v + 1

# Cause-Effect Component
from spacy.matcher import Matcher
cause_eff_matcher = Matcher(nlp.vocab)

#defining rules to detect Cause-Effect relation examples
#handling Cause-Effect patterns:
# Cause-Effect Patterns:
ce_pattern1 = [{'LEMMA': 'cause'}]
ce_pattern2 = [{'LEMMA': 'from'}]
ce_pattern3 = [{'LEMMA': 'from'}, {'POS': 'DET'}]
ce_pattern4 = [{'POS': 'VERB'}, {'LEMMA': 'by'}]
ce_pattern5 = [{'POS': 'VERB'}, {'LEMMA': 'to'}]

cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern1)
cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern2)
cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern3)
cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern4)
cause_eff_matcher.add('CAUSE_EFFECT_PATTERN', None, ce_pattern5)

v = 0
for var in in_between_list:
  ce_doc = nlp(var)
  matches = cause_eff_matcher(ce_doc)
  if(len(matches)>=1):
    matches_list[v] = 'Cause-Effect'
    if(e1_dep[v] == 'compound' or e1_dep[v] == 'nsubjpass' or e1_dep[v] == 'attr' or e2_dep[v] == 'pcomp'):
      relation_direction[v] = 'Cause-Effect(e2,e1)'
    elif(e1_dep[v] == 'conj'):
      if(e2_dep[v] == 'pobj' or e2_dep[v] == 'amod' or e2_dep[v] == 'conj'):
        relation_direction[v] = 'Cause-Effect(e2,e1)'
      else:
        relation_direction[v] = 'Cause-Effect(e1,e2)'
    elif(e1_dep[v] == 'pobj'):
      if(e2_dep[v] == 'pobj' or e2_dep[v] == 'amod'):
        relation_direction[v] = 'Cause-Effect(e2,e1)'
      else:
        relation_direction[v] = 'Cause-Effect(e1,e2)'
    elif(e1_dep[v] == 'dobj'):
      if(e2_dep[v] == 'pcomp' or e2_dep[v] == 'pobj'):
        relation_direction[v] = 'Cause-Effect(e2,e1)'
      else:
        relation_direction[v] = 'Cause-Effect(e1,e2)'
    elif(e1_dep[v] == 'nsubj'):
      if(e2_dep[v] == 'pobj'):
        relation_direction[v] = 'Cause-Effect(e2,e1)'
      else:
        relation_direction[v] = 'Cause-Effect(e1,e2)'
    else:
      relation_direction[v] = 'Cause-Effect(e1,e2)'
  v = v + 1

# Entity-Destination Component
from spacy.matcher import Matcher
ent_dest_matcher = Matcher(nlp.vocab)

#defining rules to detect Entity-Destination relation examples
#handling Entity-Destination patterns:
# Entity-Destination Patterns:
ed_pattern1 = [{'POS': 'VERB'}, {'LEMMA': 'into'}]
ed_pattern2 = [{'LEMMA': 'into'}]
# ed_pattern3 = [{'POS': 'ADV'}, {'POS': 'inot'}]
# ed_pattern4 = [{'POS': 'ADP'}, {'POS': 'DET'}]


ent_dest_matcher.add('ENTITY_DESTINATION_PATTERN', None, ed_pattern1)
ent_dest_matcher.add('ENTITY_DESTINATION_PATTERN', None, ed_pattern2)
# ed_matcher.add('ENTITY_DESTINATION_PATTERN', None, ed_pattern3)

v = 0
for var in in_between_list:
  ed_doc = nlp(var)
  matches = ent_dest_matcher(ed_doc)
  if(len(matches)>=1):
    matches_list[v] = 'Entity-Destination'
    if(e1_dep[v] == 'pobj' and e2_dep[v] == 'dobj'):
      relation_direction[v] = 'Entity-Destination(e2,e1)'
    else:
      relation_direction[v] = 'Entity-Destination(e1,e2)'
  v = v + 1

# Entity-Origin Component
from spacy.matcher import Matcher
ent_orig_matcher = Matcher(nlp.vocab)

#defining rules to detect Entity-Origin relation examples
#handling Entity-Origin patterns:
# Entity-Origin Patterns:
pattern2 = [{'POS': 'VERB'}, {'LEMMA': 'from'}]
pattern3 = [{'POS': 'ADV'}, {'LEMMA': 'from'}]

ent_orig_matcher.add('PATTERN', None, pattern2)
ent_orig_matcher.add('PATTERN', None, pattern3)

v = 0
for var in in_between_list:
  doc_eo = nlp(var)
  matches = ent_orig_matcher(doc_eo)
  if(len(matches)>=1):
    matches_list[v] = 'Entity-Origin'
    if(e2_dep[v] == 'pobj' or e2_dep[v] == 'compound'):
      relation_direction[v] = 'Entity-Origin(e1,e2)'
    elif(e2_dep[v] == 'nsubj'):
      relation_direction[v] = 'Entity-Origin(e2,e1)'
    else:
      relation_direction[v] = 'Entity-Origin(e1,e2)'
  if(matches_list[v] == ''):
    matches_list[v] = 'Other'
    relation_direction[v] = 'Other'
  v = v + 1

# Setting 1: (Assuming only the relation is classified correctly)
# The actual output of the test sentence:
print("Setting 1:")
print("The actual output of the test sentence:", rel_wo_dir[0])
print("The predicted output of the test sentence:", matches_list[0])

# Setting 2: (Assuming both the relation and direction is classified correctly)
# The actual output of the test sentence:
print("Setting 2:")
print("The actual output of the test sentence:", rel[0])
print("The predicted output of the test sentence:", relation_direction[0])

# Storing the output in a csv file
import csv
with open('finaloutput_data.csv', 'w', newline='') as csvfile:
  fieldnames = ['Sentence', 'Entity1', 'E1_POS', 'E1_DEP', 'Entity2',  'E2_POS', 'E2_DEP','Relationship','Direction', 'predicted_relationship','predicted_rel_with_direction']
  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
  writer.writeheader()

  i = 0
  for p in sentences:
    writer.writerow({'Sentence': i, 'Entity1': entity1[i], 'E1_POS': e1_pos[i], 'E1_DEP': e1_dep[i], 'Entity2': entity2[i], 'E2_POS': e2_pos[i], 'E2_DEP': e2_dep[i], 'Relationship': rel_wo_dir[i], 'Direction': direction[i], 'predicted_relationship': matches_list[i], 'predicted_rel_with_direction':relation_direction[i]})
    i = i+1

from sklearn.metrics import f1_score

from sklearn.metrics import accuracy_score

from sklearn.metrics import recall_score

from sklearn.metrics import precision_score

from sklearn.metrics import classification_report

import numpy as np

target_names = ['Product-Producer', 'Cause-Effect', 'Component-Whole', 'Member-Collection', 'Message-Topic', 'Entity-Origin', 'Content-Container', 'Entity-Destination', 'Other', 'Instrument Agency']
print("Results for semeval_test data set for relation only:")
print(classification_report(rel_wo_dir, matches_list, target_names=target_names))

target_names = ['Message-Topic(e1,e2)', 'Product-Producer(e2,e1)', 'Instrument-Agency(e2,e1)', 'Entity-Destination(e1,e2)', 'Cause-Effect(e2,e1)', 'Component-Whole(e1,e2)', 'Product-Producer(e1,e2)', 'Member-Collection(e2,e1)', 'Other', 'Entity-Origin(e1,e2)', 'Content-Container(e1,e2)', 'Entity-Origin(e2,e1)', 'Cause-Effect(e1,e2)', 'Component-Whole(e2,e1)', 'Content-Container(e2,e1)', 'Instrument-Agency(e1,e2)', 'Message-Topic(e2,e1)', 'Member-Collection(e1,e2)', 'Entity-Destination(e2,e1)']
# target_names = ['Product-Producer(e1,e2)', 'Product-Producer(e2,e1)', 'Cause-Effect(e1,e2)', 'Cause-Effect(e2,e1)', 'Component-Whole(e1,e2)', 'Component-Whole(e2,e1)', 'Member-Collection(e1,e2)', 'Member-Collection(e2,e1)', 'Message-Topic(e1,e2)', 'Message-Topic(e2,e1)', 'Entity-Origin(e1,e2)', 'Entity-Origin(e2,e1)', 'Content-Container(e1,e2)', 'Content-Container(e2,e1)', 'Entity-Destination(e1,e2)', 'Entity-Destination(e2,e1)', 'Instrument Agency(e1,e2)', 'Instrument Agency(e2,e1)', 'Other']
print("Results for semeval_test data set for relation and its directionality:")
print(classification_report(rel, relation_direction, target_names=target_names))